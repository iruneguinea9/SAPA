{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URL dataset\n",
    "\n",
    "https://www.kaggle.com/datasets/ipythonx/wikiart-gangogh-creating-art-gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "dataset = load_dataset(\"huggan/wikiart\", split=\"train\")\n",
    "\n",
    "print(dataset[0][\"text\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debes convertir las imágenes a tensores y normalizarlas\n",
    "from torchvision import transforms\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),\n",
    "])\n",
    "\n",
    "def preprocess_images(examples):\n",
    "    examples[\"image\"] = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return examples\n",
    "\n",
    "dataset = dataset.map(preprocess_images, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\")\n",
    "\n",
    "def tokenize_captions(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=77)\n",
    "\n",
    "dataset = dataset.map(tokenize_captions, batched=True)\n",
    "dataset.set_format(\"torch\", columns=[\"input_ids\", \"image\"])\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning SDXL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModel\n",
    "import torch\n",
    "from diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, DDPMScheduler\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from diffusers import AutoencoderKL\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "MODEL_NAME = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "DATASET_NAME = \"huggan/wikiart\"\n",
    "OUTPUT_DIR = \"sdxl-wikiart\"\n",
    "BATCH_SIZE = 1\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_EPOCHS = 3  \n",
    "MIXED_PRECISION = \"fp16\"\n",
    "\n",
    "# Cargar y preparar datasets\n",
    "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"test\"]\n",
    "\n",
    "# Crear DataLoaders\n",
    "def collate_fn(examples):\n",
    "    return {\n",
    "        \"input_ids\": torch.stack([example[\"input_ids\"] for example in examples]),\n",
    "        \"image\": torch.stack([example[\"image\"] for example in examples])\n",
    "    }\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Cargar componentes SDXL\n",
    "unet = UNet2DConditionModel.from_pretrained(MODEL_NAME, subfolder=\"unet\")\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(MODEL_NAME, subfolder=\"scheduler\")\n",
    "vae = AutoencoderKL.from_pretrained(MODEL_NAME, subfolder=\"vae\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(MODEL_NAME, subfolder=\"text_encoder\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=LEARNING_RATE)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=NUM_EPOCHS * len(train_dataset) // (BATCH_SIZE * GRAD_ACCUM_STEPS),\n",
    ")\n",
    "\n",
    "# Preparar con Accelerator\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    mixed_precision=MIXED_PRECISION,\n",
    ")\n",
    "\n",
    "train_dataloader, val_dataloader, unet, vae, text_encoder, optimizer, lr_scheduler = accelerator.prepare(\n",
    "    train_dataloader, val_dataloader, unet, vae, text_encoder, optimizer, lr_scheduler\n",
    ")\n",
    "\n",
    "# Entrenamiento\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Fase de entrenamiento\n",
    "    unet.train()\n",
    "    total_train_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch} [Train]\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        with accelerator.accumulate(unet):\n",
    "            # Obtener hidden states del texto\n",
    "            encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "            \n",
    "            # Convertir imágenes a latents\n",
    "            latents = vae.encode(batch[\"image\"]).latent_dist.sample() * 0.18215\n",
    "            \n",
    "            # Añadir ruido\n",
    "            noise = torch.randn_like(latents)\n",
    "            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (latents.shape[0],)).long()\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "            \n",
    "            # Predecir ruido\n",
    "            noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "            \n",
    "            # Calcular pérdida\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Fase de validación\n",
    "    unet.eval()\n",
    "    total_val_loss = 0\n",
    "    val_progress = tqdm(val_dataloader, desc=f\"Epoch {epoch} [Val]\")\n",
    "    \n",
    "    for batch in val_progress:\n",
    "        with torch.no_grad():\n",
    "            encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "            latents = vae.encode(batch[\"image\"]).latent_dist.sample() * 0.18215\n",
    "            noise = torch.randn_like(latents)\n",
    "            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (latents.shape[0],)).long()\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "            noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "            val_loss = F.mse_loss(noise_pred, noise)\n",
    "            \n",
    "            total_val_loss += val_loss.item()\n",
    "            val_progress.set_postfix(val_loss=val_loss.item())\n",
    "\n",
    "    # Guardar checkpoint\n",
    "    if epoch % 1 == 0:\n",
    "        accelerator.save_state(f\"{OUTPUT_DIR}/checkpoint-{epoch}\")\n",
    "    \n",
    "    # Mostrar métricas\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    print(f\"\\nEpoch {epoch} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\\n\")\n",
    "\n",
    "# Guardar modelo final\n",
    "accelerator.wait_for_everyone()\n",
    "unet = accelerator.unwrap_model(unet)\n",
    "unet.save_pretrained(OUTPUT_DIR, safe_serialization=True)\n",
    "print(f\"✅ Modelo final guardado en: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    OUTPUT_DIR,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "prompt = \"A painting in Van Gogh style with vibrant colors\"\n",
    "image = pipe(prompt, num_inference_steps=30).images[0]\n",
    "image.save(\"van_gogh_style.png\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
