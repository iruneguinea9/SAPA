{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apuntes SAPA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición del problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un resumen breve de los objetivos del ejercicio, como implementar un algoritmo, evaluar un modelo, o entender un concepto teórico.\n",
    "Ejemplo: \"Entrenar un modelo de clasificación para predecir si un cliente comprará un producto basado en sus datos demográficos\".\n",
    "\n",
    "Librerías: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,cross_val_predict, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder,  StandardScaler, FunctionTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import set_config\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression, SGDClassifier, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score ,mean_squared_error, precision_score, recall_score, f1_score, roc_auc_score, make_scorer, roc_curve\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import joblib\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "set_config(display=\"diagram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargaremos los datos de un CSV o de un dataset de Seaborn\n",
    "\n",
    "CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_housing_data():\n",
    "    try:\n",
    "        return pd.read_csv(\"2_8/loan_data.csv\")\n",
    "    except:\n",
    "        print(\"Fichero no encontrado\")\n",
    "\n",
    "loan_data = load_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset de Seaborn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploración de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones para analizar los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head()\n",
    "dataframe.describe()\n",
    "dataframe.info()\n",
    "# Contar valores por clase\n",
    "dataframe[\"class\"].value_counts()\n",
    "# Para saber cuales son los posibles valores de class\n",
    "dataframe[\"class\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar columnas innecesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas = ['class', 'who', 'adult_male', 'deck', 'embark_town', 'alive', 'alone'] # Columnas a eliminar\n",
    "dataframe = dataframe.drop(columns=columnas, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionar solo columnas numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_numericas = dataframe.select_dtypes(include=['int64', 'float64']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionar solo columnas categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_categoricas = dataframe.select_dtypes(include=['object', 'category']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricas = ['pclass', 'sex', 'embarked', 'survived','sibsp','parch']  \n",
    "continuas = ['fare','age']  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráficos para variables categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de barras para variables categóricas\n",
    "plt.figure(figsize=(20, 12))\n",
    "for i, var in enumerate(categoricas):\n",
    "    plt.subplot(4, 2, i+1)\n",
    "    sns.countplot(x=var, data=dataframe)\n",
    "    plt.title(f'{var}')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráficos para variables continuas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de histograma para variables continuas\n",
    "plt.figure(figsize=(20, 12))\n",
    "for i, var in enumerate(continuas):\n",
    "    plt.subplot(4, 2, i+1)\n",
    "    sns.histplot(dataframe[var], kde=True)\n",
    "    plt.title(f'{var}')\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel('Frecuencia')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráficos con diferentes valores\n",
    "Ejemplo número de supervivientes según la clase de billete y por sexos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=dataframe, x='pclass', y='survived', hue='sex')\n",
    "\n",
    "# Añadir títulos y etiquetas\n",
    "plt.title('Grafico de barras para representar el número de supervivientes según la clase de billete y por sexos')\n",
    "plt.xlabel('Clase')\n",
    "plt.ylabel('Número de Supervivientes')\n",
    "plt.legend(title='Sexo')\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subplots con variables numéricas y categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar las columnas numéricas y categóricas\n",
    "numericas = dataframe.select_dtypes(include=['int64', 'float64']).columns\n",
    "categoricas = dataframe.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Configuración de estilo de gráficos\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=8)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "\n",
    "# Crear subplots para las variables numéricas\n",
    "num_cols = len(numericas)\n",
    "num_rows = Math.ceil(num_cols / 3)\n",
    "plt.figure(figsize=(18, 6 * num_rows))\n",
    "\n",
    "for i, col in enumerate(numericas, 1):\n",
    "    plt.subplot(num_rows, 3, i)\n",
    "    plt.hist(numericas = dataframe.select_dtypes(include=['int64', 'float64']).columns\n",
    "[col].dropna(), bins=40, color='blue', alpha=0.7, edgecolor='black')\n",
    "    plt.title(f'Histograma de {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Crear subplots para las variables categóricas\n",
    "cat_cols = len(categoricas)\n",
    "cat_rows = Math.ceil(cat_cols / 3)\n",
    "plt.figure(figsize=(18, 6 * cat_rows))\n",
    "\n",
    "for i, col in enumerate(categoricas, 1):\n",
    "    plt.subplot(cat_rows, 3, i)\n",
    "    sns.countplot(data=dataframe, x=col, hue=col, palette='viridis', dodge=False, order=dataframe[col].value_counts().index)\n",
    "    plt.title(f'Countplot de {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráfico de cajas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Relación entre variables clave y el pago del préstamo', fontsize=16)\n",
    "\n",
    "sns.boxplot(x='not.fully.paid', y='int.rate', data=loan_data, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Tasa de interés vs Pago completo')\n",
    "\n",
    "sns.boxplot(x='not.fully.paid', y='fico', data=loan_data, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Puntaje FICO vs Pago completo')\n",
    "\n",
    "sns.boxplot(x='not.fully.paid', y='dti', data=loan_data, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('DTI vs Pago completo')\n",
    "\n",
    "sns.countplot(x='purpose', hue='not.fully.paid', data=loan_data, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Propósito del préstamo vs Pago completo')\n",
    "axes[1, 1].set_xticklabels(axes[1, 1].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matríz de correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solo con los numericos\n",
    "correlaciones = dataframe.corr(numeric_only=True)\n",
    "# Establecer la columna objetivo\n",
    "correlaciones[\"survived\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostrar la matríz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlaciones.style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A veces es útil eliminar las columnas con poca correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_corr = correlaciones['survived'].abs()\n",
    "target_corr\n",
    "# columas com menos correlación que 0.04\n",
    "eliminar = target_corr[target_corr<0.04].index.to_list()\n",
    "# Eliminar columnas \n",
    "dataframe = dataframe.drop(columns=eliminar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividir los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para dividir los datos, tenemos que separar la columna objetivo del resto del dataframe. En X guardar todo el dataframe menos la columna objetivo, y en y guardar la columna objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataframe.drop('survived', axis=1)\n",
    "\n",
    "y = dataframe['survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después aplico la función train_test_split para separar los datos de entrenamiento de los de prueba, respentando X e y, establecemos también el test_size (0.2 es un %20 de los datos para probar), random state se usa para obtener siempre los mismos datos en el aleatorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparar los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para las categoricas, ponemos most_frecuent y aplicamos one hot encoding. Para las numéricas, ponemos la media, y escalamos los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricas_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy='most_frequent'), \n",
    "    OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    ")\n",
    "\n",
    "numericas_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'), \n",
    "    StandardScaler() \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Hay columnas que pueden necesitar más transformación (ejemplo 2_4)\n",
    " #### Crear columna a partir de otra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una columna familia que sea la suma de las columans parch y sibsp\n",
    "def crearFamilia(X):\n",
    "     X=pd.DataFrame(X,columns=['parch','sibsp'])\n",
    "     X['familia'] = X['sibsp'] + X['parch'] \n",
    "     return X['familia'].values.reshape(-1,1)\n",
    " \n",
    "# Cuando lo hacemos así, necesitamos darle un nombre de salida a la columna, si no, se queda solo con los datos\n",
    "def familia_name(function_transformer, feature_names_in):\n",
    "     return ['familia']\n",
    " \n",
    "# pipeline\n",
    "familia_pipeline = make_pipeline(\n",
    "     SimpleImputer(strategy='most_frequent'),\n",
    "     FunctionTransformer(crearFamilia,feature_names_out=familia_name,validate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valores categoricos a 1 y 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si es female 1 si no 0\n",
    "def sexNumeros(X):\n",
    "     return np.where(X == 'female',1,0)\n",
    " \n",
    "# nombre para la columna\n",
    "def formatearSex(function_transformer, feature_names_in):\n",
    "     return ['sex']\n",
    " \n",
    "# pipeline\n",
    "sex_pipeline=make_pipeline(\n",
    "     SimpleImputer(strategy=\"most_frequent\"), \n",
    "     FunctionTransformer(sexNumeros,feature_names_out=formatearSex)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rangos de valores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso queríamos esto \n",
    "- 1 - <=16\n",
    "- 2 - (16,32]\n",
    "- 3 - (32,48]\n",
    "- 4 - (48,64]\n",
    "- 5 - >64)\n",
    "Por lo que establecemos los límites en bin (np.inf es infinito), y aplicamos cut para categorizarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separar_age(X):\n",
    "     X=pd.DataFrame(X,columns=['age'])\n",
    "     X['age'] = pd.cut(X['age'], bins=[-1,16,32,48,64,np.inf], labels=[1,2,3,4,5]).to_numpy().reshape(-1,1)  \n",
    "     return X\n",
    " \n",
    "# Nombre de columna\n",
    "def age_name(function_transformer, feature_names_in):\n",
    "     return ['age']\n",
    " \n",
    "# pipeline\n",
    "age_pipeline=make_pipeline(\n",
    "     SimpleImputer(strategy='mean'),\n",
    "     FunctionTransformer(separar_age,feature_names_out=age_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables con cola larga y ceros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos la raíz cuadrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_pipeline = make_pipeline(\n",
    "     SimpleImputer(strategy=\"mean\"),\n",
    "     FunctionTransformer(np.sqrt, feature_names_out=\"one-to-one\"),\n",
    "     StandardScaler()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después debemos combinar todas estas pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_titanic = ColumnTransformer([\n",
    "     ('pclass', numericas_pipeline, ['pclass']), \n",
    "     ('sex', sex_pipeline, ['sex']),\n",
    "     ('age', age_pipeline, ['age']), \n",
    "     ('familia', familia_pipeline,['sibsp', 'parch']), \n",
    "     ('fare',fare_pipeline, ['fare']), \n",
    "     ('embarked', categoricas_pipeline, ['embarked']) \n",
    " ],\n",
    " remainder='passthrough',verbose_feature_names_out=False\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos la pipeline hecha, hacemos la transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_copy = X_test.copy()\n",
    "df_test = pipeline_titanic.fit_transform(X_test_copy)\n",
    "\n",
    "column_names = pipeline_titanic.get_feature_names_out()\n",
    "df_test = pd.DataFrame(df_test, columns=column_names)\n",
    "\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento y evaluación del modelo\n",
    "### Tipos de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Modelos de Regresión: \n",
    "Estos modelos se utilizan para predecir valores continuos, como precios, temperaturas o cantidades.\n",
    "\n",
    "- Regresión Lineal: Modela la relación entre una variable dependiente y una o más independientes mediante una línea recta.\n",
    "- Regresión Polinómica: Amplía la regresión lineal para capturar relaciones no lineales.\n",
    "- Regresión Ridge/Lasso: Variantes de regresión lineal que incluyen regularización para evitar el sobreajuste.\n",
    "\n",
    "##### 2. Modelos de Clasificación: \n",
    "Usados para categorizar datos en clases discretas.\n",
    "\n",
    "- Regresión Logística: Aunque lleva el nombre \"regresión\", es un modelo de clasificación binaria.\n",
    "- Máquinas de Soporte Vectorial (SVM): Separa datos mediante hiperplanos.\n",
    "- Árboles de Decisión y Bosques Aleatorios: Modelos basados en particiones de datos para clasificación (y regresión).\n",
    "- k-Nearest Neighbors (k-NN): Clasifica puntos según sus vecinos más cercanos.\n",
    "- Redes Neuronales Artificiales: Usadas para problemas complejos como imágenes y texto.\n",
    "\n",
    "##### 3. Modelos de Agrupamiento (Clustering):\n",
    "Estos modelos no supervisados agrupan datos según similitudes.\n",
    "\n",
    "- k-Means: Divide los datos en k grupos basados en sus características.\n",
    "- Jerárquico: Agrupa datos en un formato jerárquico.\n",
    "- DBSCAN: Detecta grupos y puntos ruidosos en datos con distribuciones arbitrarias.\n",
    "\n",
    "##### 4. Modelos de Reducción de Dimensionalidad: \n",
    "Usados para preprocesar datos antes de aplicar otros modelos.\n",
    "\n",
    "- Análisis de Componentes Principales (PCA).\n",
    "- TSNE: Representación visual para datos complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuándo usar cada modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Modelos de Regresión\n",
    "Usados cuando el objetivo es predecir un valor continuo.\n",
    "\n",
    "##### Regresión Lineal:\n",
    "\n",
    "Úsalo si los datos muestran una relación lineal clara entre las variables independientes (X) y la dependiente (y).\n",
    "Ideal para problemas simples como predecir precios o cantidades.\n",
    "Ejemplo: Predecir el precio de una casa basado en su tamaño.\n",
    "\n",
    "##### Regresión Polinómica:\n",
    "\n",
    "Úsalo si la relación entre las variables no es lineal pero sigue una tendencia suave.\n",
    "Requiere cuidado con el grado del polinomio para evitar sobreajuste.\n",
    "Ejemplo: Predicción de la trayectoria de un proyectil.\n",
    "\n",
    "##### Ridge y Lasso:\n",
    "\n",
    "Úsalo cuando hay colinealidad entre las variables independientes o demasiados atributos.\n",
    "Ayudan a prevenir sobreajuste aplicando regularización.\n",
    "Ejemplo: Modelos predictivos en datos económicos con múltiples factores.\n",
    "\n",
    "##### 2. Modelos de Clasificación\n",
    "Utilizados cuando el objetivo es asignar una clase o categoría.\n",
    "\n",
    "##### Regresión Logística:\n",
    "\n",
    "Para problemas binarios donde hay dos clases posibles (e.g., 1 o 0).\n",
    "Ideal para problemas como detección de spam.\n",
    "Ejemplo: Predecir si un cliente realizará una compra (Sí/No).\n",
    "SVM (Máquinas de Soporte Vectorial):\n",
    "\n",
    "Cuando las clases están bien separadas o deseas encontrar un margen óptimo.\n",
    "Es útil en problemas con datos de alta dimensionalidad.\n",
    "Ejemplo: Clasificación de imágenes de dígitos escritos a mano.\n",
    "\n",
    "##### Árboles de Decisión y Bosques Aleatorios:\n",
    "\n",
    "Úsalo si buscas interpretabilidad o estás trabajando con datos categóricos y numéricos mezclados.\n",
    "Los Bosques Aleatorios son ideales para datos ruidosos o con alta varianza.\n",
    "Ejemplo: Clasificación de pacientes según síntomas médicos.\n",
    "\n",
    "##### k-Nearest Neighbors (k-NN):\n",
    "\n",
    "Para problemas con pocos datos y relaciones complejas entre características.\n",
    "No requiere entrenamiento, pero puede ser costoso computacionalmente para grandes conjuntos de datos.\n",
    "Ejemplo: Recomendación de productos basada en similitudes.\n",
    "\n",
    "##### 3. Modelos de Agrupamiento (Clustering)\n",
    "Usados para explorar patrones en datos sin etiquetas.\n",
    "\n",
    "##### k-Means:\n",
    "\n",
    "Úsalo si los grupos tienen formas esféricas y la cantidad de clusters es conocida.\n",
    "Es rápido y fácil de implementar.\n",
    "Ejemplo: Agrupar clientes según comportamientos de compra.\n",
    "\n",
    "##### DBSCAN:\n",
    "\n",
    "Útil para identificar grupos de formas arbitrarias y detectar valores atípicos.\n",
    "No requiere especificar un número exacto de clusters.\n",
    "Ejemplo: Agrupamiento de puntos geográficos en mapas.\n",
    "\n",
    "##### Jerárquico:\n",
    "\n",
    "Cuando deseas una estructura de cluster jerárquica y un gráfico dendrograma para análisis.\n",
    "Escalable a datos de tamaño pequeño o moderado.\n",
    "Ejemplo: Clasificación de especies biológicas.\n",
    "\n",
    "##### 4. Modelos de Reducción de Dimensionalidad\n",
    "Usados para simplificar datos manteniendo la mayor parte de la información.\n",
    "\n",
    "##### PCA:\n",
    "\n",
    "Úsalo cuando trabajas con datos de alta dimensionalidad y deseas eliminar redundancias.\n",
    "Es muy útil como paso previo para otros modelos.\n",
    "Ejemplo: Visualización de datos complejos en 2D o 3D.\n",
    "\n",
    "##### TSNE:\n",
    "\n",
    "Ideal para visualizar datos complejos en pocas dimensiones.\n",
    "Menos utilizado para modelado directo, pero útil para análisis exploratorio.\n",
    "Ejemplo: Exploración de datos en problemas de imágenes.\n",
    "\n",
    "#### Decisión sobre el modelo\n",
    "\n",
    "##### ¿Es supervisado o no supervisado?\n",
    "\n",
    "Si tienes etiquetas (datos conocidos de salida), usa modelos supervisados (regresión o clasificación).\n",
    "Si no tienes etiquetas, usa modelos no supervisados (clustering).\n",
    "\n",
    "##### ¿Es un valor continuo o discreto?\n",
    "\n",
    "Continuo: Modelos de regresión.\n",
    "Discreto: Modelos de clasificación.\n",
    "\n",
    "##### Tamaño y tipo de datos:\n",
    "\n",
    "Grandes dimensiones: PCA para reducción antes de clasificar.\n",
    "Datos ruidosos: Bosques Aleatorios o SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelos a evaluar\n",
    "modelos = {\n",
    "    # Regresiones\n",
    "    \"Regresión Logística\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "    \"SGDRegressor\":SGDRegressor(random_state=42),\n",
    "    \"Ridge\":Ridge(random_state=42),\n",
    "    \"Lasso\":Lasso(random_state=42),\n",
    "    \"SVR\":SVR(),\n",
    "    \"Random forest\":RandomForestRegressor(random_state=42),    \n",
    "    # Clasificadores\n",
    "    \"SGDClassifier\": SGDClassifier(random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42)    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas de evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definición: Proporción de predicciones correctas sobre el total de predicciones.\n",
    "\n",
    "Uso: Útil para problemas balanceados, pero puede ser engañosa en datasets desbalanceados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Precisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Definición: Proporción de verdaderos positivos entre todos los resultados positivos predichos.\n",
    "\n",
    "Uso: Importante cuando el costo de los falsos positivos es alto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall\n",
    "Definición: Proporción de verdaderos positivos identificados correctamente.\n",
    "\n",
    "Uso: Crucial cuando el costo de los falsos negativos es alto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1-score\n",
    "Definición: Media armónica de precision y recall.\n",
    "\n",
    "Uso: Proporciona un balance entre precision y recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Técnicas de validación cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Fold Cross-Validation\n",
    "Descripción: Divide los datos en k subconjuntos, entrenando en k-1 y validando en el restante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified K-Fold\n",
    "Descripción: Similar a K-Fold, pero mantiene la proporción de clases en cada fold.\n",
    "\n",
    "Uso: Preferible para problemas de clasificación desbalanceados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nombre, modelo in modelos.items():\n",
    "    print(f\"Modelo {nombre}\")\n",
    "\n",
    "    pipeline = make_pipeline(pipeline_titanic,modelo)\n",
    "    \n",
    "    y_pred = cross_val_predict(pipeline, X_train, y_train, cv=5)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "    print(f\"- RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si calculamos el minimo y el máximo, podemos ver el porcentaje de error y decidir si es aceptable o no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_y = np.max(y)\n",
    "min_y = np.min(y)\n",
    "\n",
    "print(f\"Máximo de y: {max_y:.2f}\")\n",
    "print(f\"Mínimo de y: {min_y:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteración sobre los modelos para calcular diferentes parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre cada modelo\n",
    "for nombre, modelo in modelos.items():\n",
    "    print(f\"Evaluación de {nombre}\")\n",
    "    \n",
    "    # Crear pipeline\n",
    "    pipeline = make_pipeline(pipeline_titanic, modelo)\n",
    "    \n",
    "    # Validación cruzada: obtener predicciones\n",
    "    y_train_pred = cross_val_predict(pipeline, X_train, y_train, cv=3)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    exactitud = accuracy_score(y_train, y_train_pred)\n",
    "    precision = precision_score(y_train, y_train_pred)\n",
    "    sensibilidad = recall_score(y_train, y_train_pred)\n",
    "    f1 = f1_score(y_train, y_train_pred)\n",
    "    \n",
    "    # Imprimir resultados\n",
    "    print(f\"  - Exactitud: {exactitud:.2f}\")\n",
    "    print(f\"  - Precisión: {precision:.2f}\")\n",
    "    print(f\"  - Sensibilidad: {sensibilidad:.2f}\")\n",
    "    print(f\"  - F1-Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curva ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La curva ROC (Receiver Operating Characteristic) se utiliza principalmente para evaluar la capacidad de un modelo de clasificación binaria para distinguir entre las clases positivas y negativas. Es una herramienta fundamental para medir el rendimiento de un clasificador, especialmente cuando los datos están desequilibrados o cuando los costos de los errores varían entre falsos positivos y falsos negativos.\n",
    "\n",
    "Definición: Área bajo la curva ROC (Receiver Operating Characteristic).\n",
    "\n",
    "Uso: Mide la capacidad del modelo para distinguir entre clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cuándo se utiliza la curva ROC\n",
    "##### Problemas de clasificación binaria:\n",
    "\n",
    "Cuando tienes dos clases (positiva y negativa) y quieres evaluar el desempeño del modelo más allá de una métrica como la precisión.\n",
    "\n",
    "Ejemplo: Diagnosticar si un paciente tiene una enfermedad (positivo) o no (negativo).\n",
    "\n",
    "##### Análisis del umbral de decisión:\n",
    "\n",
    "La curva ROC evalúa cómo cambia la sensibilidad (verdaderos positivos) y la especificidad (1 - falsos positivos) al variar el umbral de clasificación.\n",
    "Útil para seleccionar un umbral óptimo según el caso de uso.\n",
    "\n",
    "##### Datos desequilibrados:\n",
    "\n",
    "Si una clase es mucho más frecuente que la otra, métricas como la precisión pueden ser engañosas. La curva ROC es robusta ante este problema.\n",
    "\n",
    "##### Comparar modelos:\n",
    "\n",
    "Permite comparar modelos en términos de su capacidad discriminativa mediante el área bajo la curva (AUC-ROC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto va dentro del bucle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar probabilidades para curva ROC\n",
    "if hasattr(modelo, 'decision_function'):\n",
    "    y_train_proba = cross_val_predict(pipeline, X_train, y_train, cv=3, method='decision_function')\n",
    "elif hasattr(modelo, 'predict_proba'):\n",
    "    y_train_proba = cross_val_predict(pipeline, X_train, y_train, cv=3, method='predict_proba')[:, 1]\n",
    "else:\n",
    "    print(f\"{nombre} no soporta cálculo de probabilidades. Se omite la curva ROC.\")\n",
    "    print(\"\")\n",
    "    #continue desxomentar esto en el bucle \n",
    "    \n",
    "# Calcular curva ROC\n",
    "fpr, tpr, _ = roc_curve(y_train, y_train_proba)\n",
    "auc = roc_auc_score(y_train, y_train_proba)\n",
    "print(f\"  - Área bajo la curva ROC (AUC): {auc:.2f}\")\n",
    "print(\"  - Curva de ROC\")\n",
    "# Graficar curva ROC\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'{nombre} (AUC = {auc:.2f})', lw=2)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', lw=1)\n",
    "plt.title(f'Curva ROC - {nombre}')\n",
    "plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matríz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict = cross_val_predict(pipeline_titanic, X_train, y_train, cv=5)\n",
    "accuracy = accuracy_score(y_train, y_train_predict)\n",
    "\n",
    "print(f\"Accuracy:{accuracy:.4f}\")\n",
    "\n",
    "matriz = confusion_matrix(y_train,y_train_predict)\n",
    "\n",
    "accuracy = accuracy_score(y_train, y_train_predict)\n",
    "precision = precision_score(y_train, y_train_predict, average='macro')\n",
    "recall = recall_score(y_train, y_train_predict, average='macro')\n",
    "f1 = f1_score(y_train, y_train_predict, average='macro')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "sns.heatmap(matriz, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Valor real')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Exactitud: {accuracy:.4f}\")\n",
    "print(f\"Precisión: {precision:.4f}\")\n",
    "print(f\"Sensibilidad: {recall:.4f}\")\n",
    "print(f\"F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimización de modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_param_grid = {\n",
    "    \"linearregression__fit_intercept\": [True, False],\n",
    "    \"linearregression__normalize\": [True, False]\n",
    "}\n",
    "\n",
    "lr_pipeline = make_pipeline(pipeline_titanic, StandardScaler(), LinearRegression())\n",
    "\n",
    "lr_grid_search = GridSearchCV(\n",
    "    lr_pipeline, lr_param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Espacio de hiperparámetros para RandomizedSearchCV\n",
    "param_grid = {\n",
    "    \"logisticregression__C\": [0.01, 0.1, 1, 10, 100],  # Regularización\n",
    "    \"logisticregression__penalty\": [\"l2\", None],  # Penalty válido (None o l2)\n",
    "    \"logisticregression__solver\": [\"lbfgs\", \"saga\"]  # Solvers compatibles\n",
    "}\n",
    "\n",
    "# Crear el pipeline con el modelo base (Regresión Logística en este caso)\n",
    "pipeline = make_pipeline(pipeline_titanic, LogisticRegression(random_state=42, max_iter=1000))\n",
    "\n",
    "# Configuración de RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,  # Número de combinaciones aleatorias a probar\n",
    "    scoring=make_scorer(f1_score),  # Optimizar F1-Score\n",
    "    cv=3,  # Validación cruzada de 3 pliegues\n",
    "    random_state=42,  # Reproducibilidad\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Usar todos los núcleos disponibles\n",
    ")\n",
    "\n",
    "# Ajustar la búsqueda en los datos de entrenamiento\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Resultados de la búsqueda\n",
    "print(\"Mejores hiperparámetros:\", random_search.best_params_)\n",
    "print(\"Mejor F1-Score en validación cruzada:\", random_search.best_score_)\n",
    "\n",
    "# Guardar el mejor modelo encontrado\n",
    "mejor_modelo = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Clasifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un pipeline con make_pipeline\n",
    "pipeline = make_pipeline(pipeline_titanic,RandomForestClassifier())\n",
    "\n",
    "param_dist = {\n",
    "    'randomforestclassifier__n_estimators': randint(10, 200),\n",
    "    'randomforestclassifier__max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Crear el objeto de búsqueda aleatoria\n",
    "random_search = RandomizedSearchCV(estimator=pipeline, param_distributions=param_dist, scoring='accuracy', cv=5)\n",
    "\n",
    "# Ajustar el modelo a los datos de entrenamiento\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Mejor estimador encontrado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        \"preprocessing__log__functiontransformer__func\": [np.log, np.log1p],\n",
    "        \"random_forest__max_features\": [4, 6, 8, 10],\n",
    "    }\n",
    "]\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='neg_root_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_param_grid = {\n",
    "    \"lasso__alpha\": np.logspace(-4, 4, 20),\n",
    "    \"lasso__max_iter\": [1000, 5000, 10000]\n",
    "}\n",
    "\n",
    "lasso_pipeline = make_pipeline(pipeline_titanic, StandardScaler(), Lasso(random_state=42))\n",
    "\n",
    "lasso_grid_search = GridSearchCV(\n",
    "    lasso_pipeline, lasso_param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNeighbors (Regresor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_regressor_param_grid = {\n",
    "    \"kneighborsregressor__n_neighbors\": range(1, 31),\n",
    "    \"kneighborsregressor__weights\": ['uniform', 'distance'],\n",
    "    \"kneighborsregressor__metric\": ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "knn_regressor_pipeline = make_pipeline(pipeline_titanic, StandardScaler(), KNeighborsRegressor())\n",
    "\n",
    "knn_regressor_random_search = RandomizedSearchCV(\n",
    "    knn_regressor_pipeline, knn_regressor_param_grid, n_iter=20, cv=3, \n",
    "    scoring='neg_mean_squared_error', random_state=42, n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNeighbors (Clasifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classifier_param_grid = {\n",
    "    \"kneighborsclassifier__n_neighbors\": range(1, 31),\n",
    "    \"kneighborsclassifier__weights\": ['uniform', 'distance'],\n",
    "    \"kneighborsclassifier__metric\": ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "knn_classifier_pipeline = make_pipeline(pipeline_titanic, StandardScaler(), KNeighborsClassifier())\n",
    "\n",
    "knn_classifier_random_search = RandomizedSearchCV(\n",
    "    knn_classifier_pipeline, knn_classifier_param_grid, n_iter=20, cv=3, \n",
    "    scoring=make_scorer(f1_score), random_state=42, n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_param_grid = {\n",
    "    \"kmeans__n_clusters\": range(2, 11),\n",
    "    \"kmeans__init\": ['k-means++', 'random'],\n",
    "    \"kmeans__max_iter\": [100, 200, 300],\n",
    "    \"kmeans__n_init\": [10, 20, 30]\n",
    "}\n",
    "\n",
    "kmeans_pipeline = make_pipeline(pipeline_titanic, StandardScaler(), KMeans(random_state=42))\n",
    "\n",
    "kmeans_random_search = RandomizedSearchCV(\n",
    "    kmeans_pipeline, kmeans_param_grid, n_iter=20, cv=3, scoring='neg_mean_squared_error', random_state=42, n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_param_grid = {\n",
    "    \"kneighborsclassifier__n_neighbors\": range(1, 31),\n",
    "    \"kneighborsclassifier__weights\": ['uniform', 'distance'],\n",
    "    \"kneighborsclassifier__metric\": ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "knn_pipeline = make_pipeline(pipeline_titanic, StandardScaler(), KNeighborsClassifier())\n",
    "\n",
    "knn_random_search = RandomizedSearchCV(\n",
    "    knn_pipeline, knn_param_grid, n_iter=20, cv=3, scoring=make_scorer(f1_score), random_state=42, n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Árbol de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_param_grid = {\n",
    "    \"decisiontreeclassifier__max_depth\": [None] + list(range(5, 31, 5)),\n",
    "    \"decisiontreeclassifier__min_samples_split\": range(2, 11),\n",
    "    \"decisiontreeclassifier__min_samples_leaf\": range(1, 11)\n",
    "}\n",
    "\n",
    "dt_pipeline = make_pipeline(pipeline_titanic, DecisionTreeClassifier(random_state=42))\n",
    "\n",
    "dt_grid_search = GridSearchCV(\n",
    "    dt_pipeline, dt_param_grid, cv=3, scoring=make_scorer(f1_score), n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_param_grid = {\n",
    "    \"svc__C\": np.logspace(-3, 3, 7),\n",
    "    \"svc__kernel\": ['rbf', 'poly', 'sigmoid'],\n",
    "    \"svc__gamma\": ['scale', 'auto'] + list(np.logspace(-3, 3, 5))\n",
    "}\n",
    "\n",
    "svm_pipeline = make_pipeline(pipeline_titanic, StandardScaler(), SVC(random_state=42))\n",
    "\n",
    "svm_random_search = RandomizedSearchCV(\n",
    "    svm_pipeline, svm_param_grid, n_iter=20, cv=3, scoring=make_scorer(f1_score), random_state=42, n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = Ridge()\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 1.0, 10.0, 100.0, 200.0]\n",
    "}\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    modelo,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalucación en el conjunto de prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluar el mejor modelo en el conjunto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mejor_modelo = grid_search.best_estimator_\n",
    "y_pred = cross_val_predict(mejor_modelo, X_test, y_test, cv=5)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"- RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con métricas y ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = mejor_modelo.predict(X_test)\n",
    "\n",
    "# Calcular métricas\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Evaluación en el conjunto de pruebas:\")\n",
    "print(f\"  - Exactitud: {test_accuracy:.2f}\")\n",
    "print(f\"  - Precisión: {test_precision:.2f}\")\n",
    "print(f\"  - Sensibilidad: {test_recall:.2f}\")\n",
    "print(f\"  - F1-Score: {test_f1:.2f}\")\n",
    "\n",
    "# Evaluar el área bajo la curva ROC (AUC)\n",
    "if hasattr(mejor_modelo.named_steps['logisticregression'], 'predict_proba'):\n",
    "    y_test_proba = mejor_modelo.predict_proba(X_test)[:, 1]\n",
    "    test_roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "    print(f\"  - Área bajo la curva ROC (AUC): {test_roc_auc:.2f}\")\n",
    "    \n",
    "    # Calcular la curva ROC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_test_proba)\n",
    "    \n",
    "    # Hacer el gráfico\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {test_roc_auc:.2f}', lw=2)\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', lw=1)\n",
    "    plt.title('Curva ROC - Modelo')\n",
    "    plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
    "    plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"El modelo no soporta cálculo de probabilidades para la curva ROC.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentación de modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(mejor_modelo, \"./modelo.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
