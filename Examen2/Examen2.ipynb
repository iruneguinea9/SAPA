{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMEN 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada error u omisión en un ejercicio resta 0.5 puntos.\n",
    "\n",
    "Cada código demás (que no se ha pedido) se interpretará como que no se está seguro de lo que se pide y resta 0.25 puntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# TODO Borrar luego lo que no use!!\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ResNet101V2\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import math\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import ultralytics \n",
    "\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from ultralytics.solutions.solutions import BaseSolution\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# TODO Borrar luego lo que no use!!\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications import ResNet101V2\n",
    "from keras.applications.resnet_v2 import preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de datos cats_vs_dogs https://www.tensorflow.org/datasets/catalog/cats_vs_dogs?hl=es-419 es un conjunto de imágenes de perros y gatos.\n",
    "\n",
    "Tienes que entrenar un modelo de redes convolucionales para que aprenda a clasificar entre perro y gato, pero no vamos a empezar de cero, vamos a usar alguna de las redes ya existentes: InceptionV3.\n",
    "\n",
    "Tienes que seguir los siguientes pasos:\n",
    "* Carga el conjunto de imágenes (entrenamiento, validación y pruebas)\n",
    "* Explora los datos: número de elementos, número de clases, nombres de las clases y dibuja algunas imágenes con sus etiquetas...\n",
    "* Prepara los datos: añade más elementos al conjunto de entrenamiento (rotando, trasladando... las imágenes actuales) y prepara los datos para el modelo.\n",
    "* Entrena un modelo para estos datos usando InceptionV3 y muestra todas las capas del modelo.\n",
    "* Busca una imagen en Internet y haz una predicción.\n",
    "\n",
    "Puedes usar take(10) a la hora de entrenar aunque los resultados sean malísimos, con el fin de que no pierdas tiempo. Usa pocos epoch y añade como comentarios que valores pondrías realmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir parámetros\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "DATASET_DIR = \"/home/iabd/Escritorio/SAPA/SAPA/Examen2/Gatosyperros/PetImages\"\n",
    "\n",
    "# Cargar el dataset completo desde el directorio\n",
    "dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATASET_DIR,\n",
    "    seed=123,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dividir el dataset en entrenamiento, validación y prueba\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = int(0.15 * len(dataset))\n",
    "\n",
    "# lo que pondría\n",
    "#train_dataset = dataset.take(train_size)\n",
    "# Para no perder tiempo\n",
    "train_dataset = dataset.take(10)\n",
    "val_dataset = dataset.skip(train_size).take(val_size)\n",
    "test_dataset = dataset.skip(train_size + val_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombres de clases\n",
    "class_names = dataset.class_names\n",
    "print(f\"Nombres de clases: {class_names}\")\n",
    "num_classes = len(class_names)\n",
    "print(f\"Número de clases: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Número de ejemplos de prueba: {test_size}\")\n",
    "print(f\"Número de ejemplos de validación: {val_size}\")\n",
    "print(f\"Número de ejemplos de entrenamiento: {train_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(dataset, num_images=6):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for images, labels in dataset.take(1):\n",
    "        for i in range(num_images):\n",
    "            plt.subplot(2, 3, i+1)\n",
    "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "            plt.title(class_names[labels[i].numpy()])\n",
    "            plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"imagenes de entrenamiento\")\n",
    "show_images(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento y aumento de datos\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip('horizontal'),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "    tf.keras.layers.RandomZoom(0.2),\n",
    "    tf.keras.layers.RandomContrast(0.2),\n",
    "    tf.keras.layers.RandomTranslation(height_factor=0.2, width_factor=0.2),\n",
    "    tf.keras.layers.RandomBrightness(0.2),\n",
    "    tf.keras.layers.RandomCrop(IMG_SIZE[0], IMG_SIZE[1])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image, label):\n",
    "    image = tf.image.resize(image, IMG_SIZE)\n",
    "    image = tf.keras.applications.inception_v3.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "# Aplicar preprocesamiento y aumento de datos al conjunto de entrenamiento\n",
    "train_dataset_augmented = train_dataset.map(lambda image, label: (data_augmentation(image), label))\n",
    "train_dataset_augmented = train_dataset_augmented.map(preprocess_image)\n",
    "\n",
    "train_dataset_preprocessed = train_dataset.map(preprocess_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar los datos originales y aumentados\n",
    "train_dataset = train_dataset_preprocessed.concatenate(train_dataset_augmented)\n",
    "train_dataset = train_dataset.shuffle(1000).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Aplicar preprocesamiento y batching a los conjuntos de validación y prueba\n",
    "val_dataset = val_dataset.map(preprocess_image).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.map(preprocess_image).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo base InceptionV3\n",
    "base_model = tf.keras.applications.InceptionV3(input_shape=(224, 224, 3),\n",
    "                                               include_top=False, weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir el modelo completo añadiendo capas adicionales al modelo base\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,  # Modelo base MobileNetV2 sin las capas superiores\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),  # Capa de pooling global para reducir la dimensionalidad\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')  # Capa de salida con 2\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congelar las capas del modelo base\n",
    "base_model.trainable = False\n",
    "\n",
    "# Compilar el modelo especificando el optimizador, la función de pérdida y las métricas a evaluar\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo con las capas del modelo base congeladas (solo se entrenarán las capas añadidas)\n",
    "history = model.fit(train_dataset, epochs=EPOCHS, validation_data=val_dataset)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "print(f\"Loss en el conjunto de prueba: {test_loss}\")\n",
    "print(f\"Precisión en el conjunto de prueba: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de la imagen de perro\n",
    "img_path = \"/home/iabd/Escritorio/SAPA/SAPA/Examen2/Datos/rin.jpg\"\n",
    "\n",
    "# Cargar la imagen\n",
    "img = Image.open(img_path)\n",
    "img = img.resize((IMG_SIZE, IMG_SIZE))\n",
    "img_array = np.array(img)\n",
    "\n",
    "# Aplicar preprocess_input del modelo correspondiente\n",
    "img_array = tf.keras.applications.mobilenet_v2.preprocess_input(img_array)\n",
    "\n",
    "# Añadir una dimensión de batch al tensor de la imagen\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# Hacer la predicción\n",
    "pred = model.predict(img_array)\n",
    "predicted_class = class_names[np.argmax(pred)]\n",
    "\n",
    "# Mostrar imagen con la predicción\n",
    "plt.imshow(img)\n",
    "plt.title(f\"Predicción: {predicted_class}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tienes que detectar cuantos coches cruzan la línea. Tienes el video origen (trafico.mp4) así como el video de como quiero que quede (trafico_resultado.mp4), fíjate las etiquetas de los objetos y donde está la línea para contar los objetos.\n",
    "\n",
    "Tienes que guardar en un archivo .mp4 el video que se crear y tienes que escribir en la terminal el marcador final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## LAS CLASES ##########\n",
    "\n",
    "from ultralytics.solutions.solutions import BaseSolution\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "class ObjectCounter(BaseSolution):\n",
    "    \"\"\"\n",
    "    A class to manage the counting of objects in a real-time video stream based on their tracks.\n",
    "\n",
    "    This class extends the BaseSolution class and provides functionality for counting objects moving in and out of a\n",
    "    specified region in a video stream. It supports both polygonal and linear regions for counting.\n",
    "\n",
    "    Attributes:\n",
    "        in_count (int): Counter for objects moving inward.\n",
    "        out_count (int): Counter for objects moving outward.\n",
    "        counted_ids (List[int]): List of IDs of objects that have been counted.\n",
    "        saved_ids (List[int]): List of IDs of objects that have been saved to the CSV file.\n",
    "        classwise_counts (Dict[str, Dict[str, int]]): Dictionary for counts, categorized by object class.\n",
    "        region_initialized (bool): Flag indicating whether the counting region has been initialized.\n",
    "        show_in (bool): Flag to control display of inward count.\n",
    "        show_out (bool): Flag to control display of outward count.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"Initializes the ObjectCounter class for real-time object counting in video streams.\"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.in_count = 0  # Contador para objetos que se acercan\n",
    "        self.out_count = 0  # Contador para objetos que se alejan\n",
    "        self.counted_ids = []  # Lista para IDs de los objetos ya contados\n",
    "        self.saved_ids = []  # Lista de IDs ya guardados en el csv\n",
    "        self.classwise_counts = {}  # Diccionario de conteo, categorizado\n",
    "        self.region_initialized = False  # Booleno para inicialización de región\n",
    "\n",
    "        self.show_in = self.CFG.get(\"show_in\", True)\n",
    "        self.show_out = self.CFG.get(\"show_out\", True)\n",
    "\n",
    "    def save_label_to_csv(self, track_id, label, action):\n",
    "        \"\"\"Save the label, track_id, action, and current time to a new CSV file with the current date.\"\"\"\n",
    "        if track_id in self.saved_ids:\n",
    "            return  # Ignora guardar el ID si ya está en la lista\n",
    "\n",
    "        current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d\")  # Fecha actual en el formato correcto\n",
    "\n",
    "        # Crea nombre del fichero con la fecha actual\n",
    "        filename = f'tracked_objects_{current_date}.csv'\n",
    "\n",
    "        # Comprueba que exista el fichero, si existe escribe otro nombre\n",
    "        file_exists = os.path.isfile(filename)\n",
    "\n",
    "        with open(filename, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "\n",
    "            # Si no existe el fichero escribe headers\n",
    "            if not file_exists:\n",
    "                writer.writerow(['track_id', 'label', 'action', 'date', 'time'])\n",
    "\n",
    "            # Escribe en la columna de datos con el ID actual\n",
    "            writer.writerow([track_id, label, action, current_time.split()[0], current_time.split()[1]])\n",
    "            self.saved_ids.append(track_id)  # Marca el ID como guardado\n",
    "\n",
    "    def count_objects(self, current_centroid, track_id, prev_position, cls):\n",
    "        \"\"\"\n",
    "        Counts objects within a polygonal or linear region based on their tracks.\n",
    "\n",
    "        Args:\n",
    "            current_centroid (Tuple[float, float]): Current centroid values in the current frame.\n",
    "            track_id (int): Unique identifier for the tracked object.\n",
    "            prev_position (Tuple[float, float]): Last frame position coordinates (x, y) of the track.\n",
    "            cls (int): Class index for classwise count updates.\n",
    "        \"\"\"\n",
    "        if prev_position is None or track_id in self.counted_ids:\n",
    "            return\n",
    "\n",
    "        action = None  # Por defecto es None\n",
    "\n",
    "        if len(self.region) == 2:  # Region lineal\n",
    "            line = self.LineString(self.region)\n",
    "            if line.intersects(self.LineString([prev_position, current_centroid])):\n",
    "                if abs(self.region[0][0] - self.region[1][0]) < abs(self.region[0][1] - self.region[1][1]):\n",
    "                    if current_centroid[0] > prev_position[0]:  # Movimiento derecha\n",
    "                        self.in_count += 1\n",
    "                        self.classwise_counts[self.names[cls]][\"IN\"] += 1\n",
    "                        action = \"IN\"\n",
    "                    else:  # Movimiento izquierda\n",
    "                        self.out_count += 1\n",
    "                        self.classwise_counts[self.names[cls]][\"OUT\"] += 1\n",
    "                        action = \"OUT\"\n",
    "                else:\n",
    "                    if current_centroid[1] > prev_position[1]:  # Movimiento abajo\n",
    "                        self.in_count += 1\n",
    "                        self.classwise_counts[self.names[cls]][\"IN\"] += 1\n",
    "                        action = \"IN\"\n",
    "                    else:  # Movimiento arriba\n",
    "                        self.out_count += 1\n",
    "                        self.classwise_counts[self.names[cls]][\"OUT\"] += 1\n",
    "                        action = \"OUT\"\n",
    "                self.counted_ids.append(track_id)\n",
    "\n",
    "        elif len(self.region) > 2:  # Region poligonal\n",
    "            polygon = self.Polygon(self.region)\n",
    "            if polygon.contains(self.Point(current_centroid)):\n",
    "                region_width = max([p[0] for p in self.region]) - min([p[0] for p in self.region])\n",
    "                region_height = max([p[1] for p in self.region]) - min([p[1] for p in self.region])\n",
    "\n",
    "                if region_width < region_height:\n",
    "                    if current_centroid[0] > prev_position[0]:  # Movimiento derecha\n",
    "                        self.in_count += 1\n",
    "                        self.classwise_counts[self.names[cls]][\"IN\"] += 1\n",
    "                        action = \"IN\"\n",
    "                    else:  # Movimiento izquierda\n",
    "                        self.out_count += 1\n",
    "                        self.classwise_counts[self.names[cls]][\"OUT\"] += 1\n",
    "                        action = \"OUT\"\n",
    "                else:\n",
    "                    if current_centroid[1] > prev_position[1]:  # Movimiento abajo\n",
    "                        self.in_count += 1\n",
    "                        self.classwise_counts[self.names[cls]][\"IN\"] += 1\n",
    "                        action = \"IN\"\n",
    "                    else:  # Movimiento arriba\n",
    "                        self.out_count += 1\n",
    "                        self.classwise_counts[self.names[cls]][\"OUT\"] += 1\n",
    "                        action = \"OUT\"\n",
    "                self.counted_ids.append(track_id)\n",
    "\n",
    "        # Guarda la etiqueta y la acción\n",
    "        if action:\n",
    "            label = f\"{self.names[cls]} ID: {track_id}\"\n",
    "            self.save_label_to_csv(track_id, label, action)\n",
    "\n",
    "    def store_classwise_counts(self, cls):\n",
    "        \"\"\"Initialize class-wise counts for a specific object class if not already present.\"\"\"\n",
    "        if self.names[cls] not in self.classwise_counts:\n",
    "            self.classwise_counts[self.names[cls]] = {\"IN\": 0, \"OUT\": 0}\n",
    "\n",
    "    def display_counts(self, im0):\n",
    "        \"\"\"Displays object counts on the input image or frame.\"\"\"\n",
    "        labels_dict = {\n",
    "            str.capitalize(key): f\"{'IN ' + str(value['IN']) if self.show_in else ''} \"\n",
    "            f\"{'OUT ' + str(value['OUT']) if self.show_out else ''}\".strip()\n",
    "            for key, value in self.classwise_counts.items()\n",
    "            if value[\"IN\"] != 0 or value[\"OUT\"] != 0\n",
    "        }\n",
    "\n",
    "        if labels_dict:\n",
    "            self.annotator.display_analytics(im0, labels_dict, (104, 31, 17), (255, 255, 255), 10)\n",
    "\n",
    "        for track_id in self.track_ids:\n",
    "            if track_id in self.counted_ids:\n",
    "                in_count = self.in_count\n",
    "                label = f\"ID:{track_id} count at number {in_count}\"\n",
    "                self.annotator.box_label(self.boxes[self.track_ids.index(track_id)], label=label, color=(255, 255, 0))\n",
    "\n",
    "    def count(self, im0):\n",
    "        \"\"\"Processes input data (frames or object tracks) and updates object counts.\"\"\"\n",
    "        if not self.region_initialized:\n",
    "            self.initialize_region()\n",
    "            self.region_initialized = True\n",
    "\n",
    "        self.annotator = Annotator(im0, line_width=self.line_width)\n",
    "        self.extract_tracks(im0)\n",
    "        self.annotator.draw_region(reg_pts=self.region, color=(104, 0, 123), thickness=self.line_width * 2)\n",
    "\n",
    "        for box, track_id, cls in zip(self.boxes, self.track_ids, self.clss):\n",
    "            self.store_tracking_history(track_id, box)\n",
    "            self.store_classwise_counts(cls)\n",
    "\n",
    "            label = f\"{self.names[cls]} ID: {track_id}\"\n",
    "            self.annotator.box_label(box, label=label, color=colors(cls, True))\n",
    "\n",
    "            current_centroid = ((box[0] + box[2]) / 2, (box[1] + box[3]) / 2)\n",
    "            prev_position = self.track_history[track_id][-2] if len(self.track_history[track_id]) > 1 else None\n",
    "            self.count_objects(current_centroid, track_id, prev_position, cls)\n",
    "\n",
    "        self.display_counts(im0)\n",
    "        self.display_output(im0)\n",
    "\n",
    "        return im0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para el tracking del ratón\n",
    "def RGB(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_MOUSEMOVE:  # Comprueba que se mueva el ratón\n",
    "        point = [x, y]\n",
    "        print(f\"Mouse moved to: {point}\")\n",
    "\n",
    "# Abrir el video\n",
    "cap = cv2.VideoCapture(\"/home/iabd/Escritorio/SAPA/SAPA/Examen2/trafico.mp4\")\n",
    "\n",
    "# Define región para contar\n",
    "region_points = [(386, 103), (458, 499)]\n",
    "\n",
    "# Inicializa el contador\n",
    "counter = ObjectCounter(\n",
    "    region=region_points,  # Pasar los puntos de la región\n",
    "    model=\"Datos/yolo11s.pt\",  # Modelo que cuente objetos\n",
    "    classes=[2],  # Fijar la detección de clases a coches (clase 2 en COCO dataset)\n",
    "    show_in=True,  # Enseñar contador de entradas\n",
    "    show_out=True,  # Enseñar contador de salidas\n",
    "    line_width=2,  # Ajustar ancho de línea para el output\n",
    ")\n",
    "\n",
    "# Crear una ventana con nombre y activar detección de ratón\n",
    "cv2.namedWindow('RGB')\n",
    "cv2.setMouseCallback('RGB', RGB)\n",
    "\n",
    "# Obtener propiedades del video\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "# Inicializar el escritor de video\n",
    "out = cv2.VideoWriter(\"/home/iabd/Escritorio/SAPA/SAPA/Examen2/trafico_resultado.mp4\", fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "count = 0\n",
    "\n",
    "while True:\n",
    "    # Leer fotograma del video\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    count += 1\n",
    "    if count % 2 != 0:  # Saltar fotogramas impares\n",
    "        continue\n",
    "\n",
    "    frame = cv2.resize(frame, (frame_width, frame_height))\n",
    "\n",
    "    # Procesar el fotograma con el contador de objetos\n",
    "    with torch.no_grad():\n",
    "        frame = counter.count(frame)\n",
    "\n",
    "    # Escribir el fotograma procesado en el archivo de salida\n",
    "    out.write(frame)\n",
    "\n",
    "    # Enseñar el fotograma\n",
    "    cv2.imshow(\"RGB\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):  # Pulsar q para salir\n",
    "        break\n",
    "\n",
    "# Liberar el capturador de video y cerrar la ventana\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Imprimir el marcador final\n",
    "print(f\"Total de coches que cruzar la línea: {counter.in_count + counter.out_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO 3\n",
    "A partir de los datos proporcionados:\n",
    "* Crea dos conjuntos de datos: train y test\n",
    "* Crea el modelo que tienes en la imagen modelo.png\n",
    "* Muestra en una imagen el modelo creado.\n",
    "* Entrena el modelo probando diferentes tasas de aprendizaje usando detención temprana y evalúa cada modelo en el conjunto de test (en el entrenamiento indica que tiene que coger el 5% para el conjunto de validación).\n",
    "* Guarda el modelo cuya media de las métricas de las tres salidas en el conjunto de test sea la mejor.\n",
    "* Indica cuál es la tasa de aprendizaje del modelo elegido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar datos artificiales\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(5000, 15) * 15  # 5000 muestras con 15 características entre 0 y 15\n",
    "X1 = X[:, :10]  # Primera entrada con 10 características\n",
    "X2 = X[:, 5:]  # Segunda entrada con 10 características solapadas\n",
    "\n",
    "# Agregar ruido aleatorio a Y\n",
    "noise = np.random.normal(0, 0.1, (5000, 3))\n",
    "Y1 = np.sin(np.sum(X1, axis=1)) + np.cos(np.sum(X2, axis=1)) + noise[:, 0]\n",
    "Y2 = np.tanh(np.prod(X1, axis=1) * np.prod(X2, axis=1)) + noise[:, 1]\n",
    "Y3 = np.log1p(np.mean(X1, axis=1) + np.mean(X2, axis=1)) + noise[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Crear conjuntos de datos de entrenamiento y prueba\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, np.column_stack((Y1, Y2, Y3)), test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir entradas\n",
    "input_dcha = tf.keras.layers.Input(shape=(10,), name=\"Input_Dcha\")\n",
    "input_izda = tf.keras.layers.Input(shape=(10,), name=\"Input_Izda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rama centro\n",
    "rama_ct_norm= tf.keras.layers.Normalization()(input_dcha)\n",
    "rama_ct= tf.keras.layers.Dense(100, activation='relu')(rama_ct_norm)\n",
    "rama_ct_dense= tf.keras.layers.Dense(100, activation='relu')(rama_ct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rama derecha\n",
    "rama_dc = tf.keras.layers.Dense(100, activation='relu')(rama_ct_norm)\n",
    "rama_dc = tf.keras.layers.Dense(100, activation='relu')(rama_dc)\n",
    "rama_dc = tf.keras.layers.Dense(100, activation='relu')(rama_dc)\n",
    "rama_dc = tf.keras.layers.Dropout(0.5)(rama_dc)\n",
    "rama_dc = tf.keras.layers.Concatenate()([rama_dc, rama_ct_dense])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rama izquierda\n",
    "\n",
    "rama_iz = tf.keras.layers.Normalization()(input_izda)\n",
    "rama_iz = tf.keras.layers.Dense(200, activation='relu')(rama_iz)\n",
    "rama_iz = tf.keras.layers.Dense(200, activation='relu')(rama_iz)\n",
    "rama_iz = tf.keras.layers.BatchNormalization()(rama_iz)\n",
    "rama_iz = tf.keras.layers.Dense(100, activation='relu')(rama_iz)\n",
    "rama_iz = tf.keras.layers.BatchNormalization()(rama_iz)\n",
    "rama_iz = tf.keras.layers.Dense(100, activation='relu')(rama_iz)\n",
    "rama_iz = tf.keras.layers.Concatenate()([rama_iz, rama_ct_dense])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salida_dcha = tf.keras.layers.Dense(1, name=\"salida_dcha\")(rama_dc)\n",
    "salida_centro = tf.keras.layers.Dense(1, name=\"salida_centro\")(rama_ct_dense)\n",
    "salida_izda = tf.keras.layers.Dense(1, name=\"salida_izda\")(rama_iz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelo\n",
    "model = tf.keras.Model(inputs=[input_dcha, input_izda], outputs=[salida_dcha, salida_centro, salida_izda])\n",
    "\n",
    "# Resumen del modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, \"modelo_irune.png\", show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir diferentes tasas de aprendizaje para probar\n",
    "learning_rates = [0.001, 0.0001, 0.00001]\n",
    "\n",
    "# Definir la detención temprana\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Almacenar los resultados de cada modelo\n",
    "results = {}\n",
    "best_model = None\n",
    "best_lr = None\n",
    "best_avg_mae = float('inf')\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"Entrenando con tasa de aprendizaje: {lr}\")\n",
    "    \n",
    "    # Crear una nueva instancia del modelo\n",
    "    model = tf.keras.Model(inputs=[input_dcha, input_izda], outputs=[salida_dcha, salida_centro, salida_izda])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='mse', metrics=['mae', 'mae', 'mae'])\n",
    "    \n",
    "    # Entrenar el modelo\n",
    "    history = model.fit(\n",
    "        [X_train[:, :10], X_train[:, 5:]], [Y_train[:, 0], Y_train[:, 1], Y_train[:, 2]],\n",
    "        validation_split=0.05,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluar el modelo en el conjunto de prueba\n",
    "    test_loss, test_mae1, test_mae2, test_mae3 = model.evaluate([X_test[:, :10], X_test[:, 5:]], [Y_test[:, 0], Y_test[:, 1], Y_test[:, 2]])\n",
    "    \n",
    "    # Calcular la media de las métricas MAE\n",
    "    avg_mae = (test_mae1 + test_mae2 + test_mae3) / 3\n",
    "    \n",
    "    # Almacenar los resultados\n",
    "    results[lr] = {\n",
    "        'history': history,\n",
    "        'test_loss': test_loss,\n",
    "        'test_mae1': test_mae1,\n",
    "        'test_mae2': test_mae2,\n",
    "        'test_mae3': test_mae3,\n",
    "        'avg_mae': avg_mae\n",
    "    }\n",
    "    \n",
    "    # Guardar el mejor modelo\n",
    "    if avg_mae < best_avg_mae:\n",
    "        best_avg_mae = avg_mae\n",
    "        best_model = model\n",
    "        best_lr = lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el mejor modelo\n",
    "best_model.save('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar los resultados\n",
    "for lr, result in results.items():\n",
    "    print(f\"Tasa de aprendizaje: {lr}\")\n",
    "    print(f\"Loss en el conjunto de prueba: {result['test_loss']}\")\n",
    "    print(f\"MAE en el conjunto de prueba (salida derecha): {result['test_mae1']}\")\n",
    "    print(f\"MAE en el conjunto de prueba (salida centro): {result['test_mae2']}\")\n",
    "    print(f\"MAE en el conjunto de prueba (salida izquierda): {result['test_mae3']}\")\n",
    "    print(f\"Media de MAE: {result['avg_mae']}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(f\"La mejor tasa de aprendizaje es: {best_lr}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
